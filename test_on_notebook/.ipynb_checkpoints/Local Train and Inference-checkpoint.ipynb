{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Deep Learning Training and HPO Code in a Local Sagemaker Instance for Dockerizing\n",
    "\n",
    "This notebook shows an example of a running Bayesian HPO and also training for a regression deep neural network written in Keras with a Tensorflow backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pickle import dump\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import hp\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Function for preparing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(train_data):   \n",
    "\n",
    "    if not final_training:\n",
    "        skip = int(100/int(used_data_percentage))\n",
    "        train_data = train_data[::skip]\n",
    "\n",
    "    train_data = train_data.dropna()\n",
    "    print(train_data.columns)\n",
    "\n",
    "    train_data = train_data.astype('float32')\n",
    "    \n",
    "    train_x = train_data.drop([target], axis=1)\n",
    "    train_y = train_data[target]\n",
    "\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = train_validation_split)\n",
    "    \n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(train_x)\n",
    "    dump(scaler, open(os.path.join(model_path, 'scaler.pkl'), 'wb'))\n",
    "    \n",
    "    train_x = scaler.transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "\n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Function for doing a final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(params):\n",
    "    input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(training_path, channel_name))\n",
    "    raw_data = [ pd.read_csv(file) for file in input_files if file.endswith('.csv')]\n",
    "    raw_data = pd.concat(raw_data)\n",
    "    train_x, train_y, test_x, test_y = data_prep(raw_data)\n",
    "    print('data loaded')    \n",
    "    start = timer()\n",
    "  \n",
    "    #######################################################\n",
    "    model = Sequential()\n",
    "    for i in range(params['num_dense_layers']-1):\n",
    "        if i ==0:\n",
    "            model.add(Dense(params['num_dense_nodes']['num_dense_nodes_1'], kernel_initializer='normal',input_dim = train_x.shape[1], activation='relu'))\n",
    "            if batch_normalization:\n",
    "                model.add(BatchNormalization())\n",
    "            if include_dropout:\n",
    "                model.add(Dropout(params['dropout']))\n",
    "        else:\n",
    "            model.add(Dense(params['num_dense_nodes']['num_dense_nodes_'+str(i+1)], kernel_initializer='normal', activation='relu'))\n",
    "            if batch_normalization:\n",
    "                model.add(BatchNormalization())\n",
    "            if include_dropout:\n",
    "                model.add(Dropout(params['dropout']))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal',activation=params['last_activation_f']))\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    model.compile(loss=loss_metric, optimizer = params['optimizer'], metrics=[loss_metric])\n",
    "    model.summary()\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor= monitor_metric, patience=early_stopping_patience, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor= monitor_metric, mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor= monitor_metric, factor=0.1, patience=lr_update_patience, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "    history = model.fit(train_x, train_y,\n",
    "              callbacks=[earlyStopping, mcp_save, reduce_lr_loss],\n",
    "              epochs=params['nb_epochs'],\n",
    "              verbose=2,\n",
    "              validation_data=(test_x, test_y))\n",
    "\n",
    "    predictions=model.predict(test_x)\n",
    "\n",
    "    df = pd.DataFrame(columns=['Actual','Predicted'])\n",
    "    df['Actual'] = test_y\n",
    "    df['Predicted'] = predictions\n",
    "    diff = abs(df['Actual']  - df['Predicted'])/df['Actual'] \n",
    "    q95 = diff.quantile(.95)\n",
    "\n",
    "\n",
    "    ###########\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(os.path.join(model_path, 'model.json'), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(os.path.join(model_path, 'model.h5'))\n",
    "    print(\"Saved model to disk\")\n",
    "    ###########\n",
    "    print('q95  {}'.format(q95))\n",
    "    run_time = timer() - start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Function for doing Bayesian HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_q95 = 10e10\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    \n",
    "    input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(training_path, channel_name))\n",
    "    raw_data = [ pd.read_csv(file) for file in input_files if file.endswith('.csv')]\n",
    "    raw_data = pd.concat(raw_data)\n",
    "\n",
    "    train_x, train_y, test_x, test_y = data_prep(raw_data)\n",
    "    print('data loaded')\n",
    "\n",
    "    global ITERATION\n",
    "    print('Iteration: {}'.format(ITERATION))\n",
    "    \n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "  \n",
    "    #######################################################\n",
    "    model = Sequential()\n",
    "\n",
    "    for i in range(params['num_dense_layers']-1):\n",
    "        if i ==0:\n",
    "            model.add(Dense(params['num_dense_nodes']['num_dense_nodes_1'], kernel_initializer='normal',input_dim = train_x.shape[1], activation='relu'))\n",
    "            if batch_normalization:\n",
    "                model.add(BatchNormalization())\n",
    "            if include_dropout:\n",
    "                model.add(Dropout(params['dropout']))\n",
    "        else:\n",
    "            model.add(Dense(params['num_dense_nodes']['num_dense_nodes_'+str(i+1)], kernel_initializer='normal', activation='relu'))\n",
    "            if batch_normalization:\n",
    "                model.add(BatchNormalization())\n",
    "            if include_dropout:\n",
    "                model.add(Dropout(params['dropout']))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal',activation= params['last_activation']))\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "    model.compile(loss=loss_metric, optimizer = params['optimizer'], metrics=[loss_metric])\n",
    "    #model.summary()\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor= monitor_metric, patience=early_stopping_patience, verbose=0, mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor= monitor_metric, factor=0.1, patience=lr_update_patience, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "    history = model.fit(train_x, train_y,\n",
    "              callbacks=[earlyStopping, reduce_lr_loss],\n",
    "              epochs=params['nb_epochs'],\n",
    "              verbose=2,\n",
    "              validation_data=(test_x, test_y))\n",
    "\n",
    "    predictions=model.predict(test_x)\n",
    "\n",
    "    df = pd.DataFrame(columns=['Actual','Predicted'])\n",
    "    df['Actual'] = test_y\n",
    "    df['Predicted'] = predictions\n",
    "    diff = abs(df['Actual']  - df['Predicted'])/df['Actual'] \n",
    "    q95 = diff.quantile(.95)\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_q95\n",
    "    global short_model_summary\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if q95 < best_q95:\n",
    "        ###########\n",
    "        # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open(os.path.join(model_path, 'model.json'), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(os.path.join(model_path, 'model.h5'))\n",
    "        \n",
    "        stringlist = []\n",
    "        model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "        short_model_summary = \"\".join(stringlist)\n",
    "\n",
    "        print(\"Saved model to disk\")\n",
    "        ###########\n",
    "        \n",
    "        # Update the regression accuracy.\n",
    "        best_q95 = q95\n",
    "    print(100*'=')\n",
    "    print(50*' ','      Iteration: \\n', ITERATION)\n",
    "    print('             q95:  \\n{}'.format(q95))\n",
    "    print('             best_q95:  \\n {}'.format(best_q95))\n",
    "    print(100*'=')\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    #######################################################    \n",
    "\n",
    "    run_time = timer() - start\n",
    "\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': q95,'params': params, 'iteration': ITERATION,\n",
    "            'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Define  HyperParameters for Training and HPO (equivalent of section 5-D in the train script)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define parameters for Final Training or HPO\n",
    "final_training = True  # This flag switched between Final Training mode (True) and HPO mode (False)\n",
    "\n",
    "if final_training: # If we are doing Final Training\n",
    "    final_training = True\n",
    "    target = 'Target'\n",
    "    batch_normalization = False\n",
    "    include_dropout = False\n",
    "    dropout_f = .2\n",
    "    early_stopping_patience = 15\n",
    "    train_validation_split = .15\n",
    "    lr_update_patience = 7\n",
    "    loss_metric = 'mae'\n",
    "    monitor_metric = 'val_mean_absolute_error'\n",
    "    num_layers_f = 8\n",
    "    nodes = [1024,64,1024,32,32,64,512] # The number of nodes (length of \"nodes\" list) should be num_layers_f-1 because the last layer has 1 node and is automatically added\n",
    "    nb_epochs_f = 3\n",
    "    batch_size_f = 32\n",
    "    optimizer_f = 'adam'\n",
    "    last_activation_f = 'tanh'\n",
    "       \n",
    "else:  # If we are doing HPO\n",
    "    final_training = False\n",
    "    target = 'Target'\n",
    "    batch_normalization = False\n",
    "    include_dropout = False\n",
    "    dropout = [.2,.3,.5]\n",
    "    early_stopping_patience = 15\n",
    "    lr_update_patience = 7\n",
    "    loss_metric = 'mae'\n",
    "    monitor_metric = 'val_mean_absolute_error'\n",
    "    used_data_percentage = 10\n",
    "    train_validation_split = .15\n",
    "    MAX_EVALS = 3\n",
    "    randstate = 50\n",
    "    num_layers_low = 1\n",
    "    num_layers_high = 9\n",
    "    choice_of_node_numbers = [16,32,64,128,256,512,1024,2048] # Here you can give the possible node size for layers. If you want to only have small number of nodes, remove the high values from this list. \n",
    "    nb_epochs = 3\n",
    "    batch_size = [32,64,128]\n",
    "    optimizer = ['adam']\n",
    "    last_activation = ['tanh']  # Activation for the layer with one node. Options for this are 'linear' and 'tanh'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- Putting above parameters in dictionaries that can be used by Training or HPO functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_training:   # If we are doing Final Training\n",
    "    parameters = {   'num_dense_layers': num_layers_f,\n",
    "                'num_dense_nodes': {'num_dense_nodes_'+str(k+1): nodes[k] for k in range(num_layers_f-1)},\n",
    "                 'batch_size' : batch_size_f,\n",
    "                'nb_epochs' :  nb_epochs_f,\n",
    "                'dropout' :  dropout_f,\n",
    "                'optimizer': optimizer_f,\n",
    "                'last_activation_f': last_activation_f\n",
    "            }\n",
    "else:    # If we are doing HPO\n",
    "    space = {   'num_dense_layers': hp.choice('num_dense_layers', np.arange(num_layers_low, num_layers_high, dtype=int)),\n",
    "                'num_dense_nodes': {'num_dense_nodes_'+str(k+1): hp.choice('num_dense_nodes_'+str(k+1), choice_of_node_numbers) for k in range(num_layers_high)},\n",
    "                 'batch_size' : hp.choice('batch_size', batch_size),\n",
    "                'nb_epochs' :  nb_epochs,\n",
    "                'optimizer': hp.choice('optimizer',optimizer),\n",
    "                'last_activation': hp.choice('last_activation',last_activation)\n",
    "            }\n",
    "\n",
    "    if include_dropout:\n",
    "        space['dropout'] = hp.choice('dropout',dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7- This is the main function which runs the final training or HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print('Starting the training/HPO.')\n",
    "    try:\n",
    "        if final_training:\n",
    "            print('Starting the final training...')\n",
    "            train_final_model(parameters)\n",
    "    \n",
    "        else:\n",
    "            print('Starting the HPO...')\n",
    "            tpe_algorithm = tpe.suggest\n",
    "            bayes_trials = Trials()\n",
    "\n",
    "            # Global variable\n",
    "            global  ITERATION\n",
    "\n",
    "            ITERATION = 0\n",
    "            # Run optimization\n",
    "            best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "                        max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(randstate))\n",
    "\n",
    "\n",
    "            print('Training is complete.')\n",
    "            # Sort the trials with lowest loss (highest AUC) first\n",
    "            print(100*'=')\n",
    "            print('\\n                 Best Model:\\n')\n",
    "            bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "            \n",
    "            print('Model Summary: \\n\\n',short_model_summary)\n",
    "            print('\\n\\n\\n')\n",
    "            print(bayes_trials_results[0])\n",
    "            print('\\n\\n\\n')\n",
    "            print(100*'=')\n",
    "            \n",
    "            print('\\n                 2nd Best Model: \\n')\n",
    "            print(bayes_trials_results[1])\n",
    "            print(100*'=')\n",
    "\n",
    "            print('\\n                 3rd Best Model: \\n')\n",
    "            print(bayes_trials_results[2])\n",
    "            print(100*'=')\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failure\n",
    "        # Reason in the DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, 'failure'), 'w') as s:\n",
    "            s.write('Exception during training: ' + str(e) + '\\n' + trc)\n",
    "        # Printing this causes the exception to be in the training job logs\n",
    "        print(\n",
    "            'Exception during training: ' + str(e) + '\\n' + trc,\n",
    "            file=sys.stderr)\n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8- Define directories for data and model artifacts (equivalent of section 8-D in the train script) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = 'data'\n",
    "output_path = 'opt/ml/output' # You can create this outside of current directory.\n",
    "model_path = 'opt/ml/model'# You can create this outside of current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9- Run train() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training/HPO.\n",
      "Starting the final training...\n",
      "Index(['F0', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n",
      "       'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20',\n",
      "       'F21', 'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'Target'],\n",
      "      dtype='object')\n",
      "data loaded\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              29696     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              66560     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                32800     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 231,617\n",
      "Trainable params: 231,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8500 samples, validate on 1500 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 0.2573 - mean_absolute_error: 0.2573 - val_loss: 0.2573 - val_mean_absolute_error: 0.2573\n",
      "Epoch 2/3\n",
      " - 1s - loss: 0.2464 - mean_absolute_error: 0.2464 - val_loss: 0.2551 - val_mean_absolute_error: 0.2551\n",
      "Epoch 3/3\n",
      " - 1s - loss: 0.2456 - mean_absolute_error: 0.2456 - val_loss: 0.2552 - val_mean_absolute_error: 0.2552\n",
      "Saved model to disk\n",
      "q95  7.131876373291015\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()\n",
    "\n",
    "    # A zero exit code causes the job to be marked a Succeeded.\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10- Define functions for local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the file that implements a flask server to do inferences. It's the\n",
    "# file that you will modify to implement the scoring for your own algorithm.\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "try:\n",
    "    from StringIO import StringIO ## for Python 2\n",
    "except ImportError:\n",
    "    from io import StringIO ## for Python 3\n",
    "    \n",
    "import flask\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pickle import load\n",
    "\n",
    "#############################\n",
    "from tensorflow import Graph, Session\n",
    "from keras import backend as K\n",
    "graph = Graph()\n",
    "\n",
    "#############################\n",
    "\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import h5py\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# prefix = '/opt/ml/'\n",
    "# model_path = os.path.join(prefix, 'model')\n",
    "\n",
    "prefix = 'opt/ml/'\n",
    "model_path = os.path.join(prefix, 'model')\n",
    "\n",
    "\n",
    "# A singleton for holding the model. This simply loads the model and holds it.\n",
    "# It has a predict function that does a prediction based on the model and the\n",
    "# input data.\n",
    "\n",
    "def loadmodel(weightFile, jsonFile):    \n",
    "    # load json and create model\n",
    "    json_file = open(jsonFile, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    reg = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    reg.load_weights(weightFile)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return reg\n",
    "   \n",
    "\n",
    "class ScoringService(object):\n",
    "    model = None                # Where we keep the model when it's loaded\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls):\n",
    "        \"\"\"\n",
    "        Get the model object for this instance,\n",
    "        loading it if it's not already loaded.\n",
    "        \"\"\"\n",
    "        if cls.model is None:\n",
    "            cls.model = loadmodel(os.path.join(model_path, 'model.h5'),os.path.join(model_path, 'model.json'))\n",
    "        return cls.model\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def predict(cls,input):\n",
    "        \"\"\"For the input, do the predictions and return them.\n",
    "\n",
    "        Args:\n",
    "            input (a pandas dataframe): The data on which to do the\n",
    "            predictions.\n",
    "\n",
    "            There will be one prediction per row in the dataframe\n",
    "        \"\"\"\n",
    "        sess = K.get_session()\n",
    "        with sess.graph.as_default():\n",
    "            clf = cls.get_model()\n",
    "            return clf.predict(input)\n",
    "\n",
    "def transform_data(dataset):\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.astype('float32')\n",
    "    scaler = load(open(os.path.join(model_path, 'scaler.pkl'), 'rb'))\n",
    "\n",
    "    # Feature Scaling\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    return pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11- Do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4617856 ],\n",
       "       [0.54397476],\n",
       "       [0.49649382],\n",
       "       [0.49941772],\n",
       "       [0.5342457 ],\n",
       "       [0.4505511 ],\n",
       "       [0.5032308 ],\n",
       "       [0.49850062],\n",
       "       [0.4888973 ],\n",
       "       [0.48980114],\n",
       "       [0.46248478],\n",
       "       [0.4839107 ],\n",
       "       [0.49577925],\n",
       "       [0.46175972],\n",
       "       [0.45104107],\n",
       "       [0.5224433 ],\n",
       "       [0.49608842],\n",
       "       [0.49281412],\n",
       "       [0.5049967 ],\n",
       "       [0.5100204 ],\n",
       "       [0.48722532],\n",
       "       [0.48989037],\n",
       "       [0.495605  ],\n",
       "       [0.51321626],\n",
       "       [0.4943084 ],\n",
       "       [0.4910444 ],\n",
       "       [0.4909398 ],\n",
       "       [0.49010822],\n",
       "       [0.4411553 ],\n",
       "       [0.48302454],\n",
       "       [0.47633272],\n",
       "       [0.49052477],\n",
       "       [0.47503665],\n",
       "       [0.51463   ],\n",
       "       [0.45878103],\n",
       "       [0.45311946],\n",
       "       [0.46756327],\n",
       "       [0.52910686],\n",
       "       [0.4847625 ],\n",
       "       [0.48792842],\n",
       "       [0.48864654],\n",
       "       [0.4519508 ],\n",
       "       [0.47349724],\n",
       "       [0.44557393],\n",
       "       [0.4562772 ],\n",
       "       [0.50370955],\n",
       "       [0.45936036],\n",
       "       [0.44813266],\n",
       "       [0.44490665],\n",
       "       [0.47032762],\n",
       "       [0.5280861 ],\n",
       "       [0.48468766],\n",
       "       [0.48786646],\n",
       "       [0.4606811 ],\n",
       "       [0.51281697],\n",
       "       [0.49612394],\n",
       "       [0.4702461 ],\n",
       "       [0.44689235],\n",
       "       [0.47799248],\n",
       "       [0.50064343],\n",
       "       [0.5376805 ],\n",
       "       [0.4735757 ],\n",
       "       [0.48688143],\n",
       "       [0.46162632],\n",
       "       [0.49292842],\n",
       "       [0.5048417 ],\n",
       "       [0.51202154],\n",
       "       [0.49875012],\n",
       "       [0.49761954],\n",
       "       [0.4873775 ],\n",
       "       [0.4756226 ],\n",
       "       [0.46843648],\n",
       "       [0.4560857 ],\n",
       "       [0.516922  ],\n",
       "       [0.4736817 ],\n",
       "       [0.4987507 ],\n",
       "       [0.47356483],\n",
       "       [0.52371836],\n",
       "       [0.460857  ],\n",
       "       [0.51079285],\n",
       "       [0.46723992],\n",
       "       [0.45056015],\n",
       "       [0.49202797],\n",
       "       [0.48798287],\n",
       "       [0.45011866],\n",
       "       [0.49467224],\n",
       "       [0.5123345 ],\n",
       "       [0.51848364],\n",
       "       [0.45801994],\n",
       "       [0.45289034],\n",
       "       [0.5209771 ],\n",
       "       [0.4543998 ],\n",
       "       [0.45934767],\n",
       "       [0.47284788],\n",
       "       [0.46303785],\n",
       "       [0.50950086],\n",
       "       [0.5529788 ],\n",
       "       [0.46010125],\n",
       "       [0.4976262 ],\n",
       "       [0.51680934],\n",
       "       [0.4713081 ],\n",
       "       [0.47264826],\n",
       "       [0.45229968],\n",
       "       [0.48534077],\n",
       "       [0.46914902],\n",
       "       [0.5085391 ],\n",
       "       [0.5935889 ],\n",
       "       [0.44788283],\n",
       "       [0.45284814],\n",
       "       [0.45724106],\n",
       "       [0.48210984],\n",
       "       [0.50224733],\n",
       "       [0.48766696],\n",
       "       [0.47918034],\n",
       "       [0.44978747],\n",
       "       [0.500544  ],\n",
       "       [0.50030756],\n",
       "       [0.4571028 ],\n",
       "       [0.47046703],\n",
       "       [0.4861274 ],\n",
       "       [0.47856122],\n",
       "       [0.4683197 ],\n",
       "       [0.48915026],\n",
       "       [0.4783866 ],\n",
       "       [0.46989134],\n",
       "       [0.46341336],\n",
       "       [0.51809794],\n",
       "       [0.45189413],\n",
       "       [0.5105428 ],\n",
       "       [0.45180464],\n",
       "       [0.48720244],\n",
       "       [0.4697443 ],\n",
       "       [0.4775925 ],\n",
       "       [0.48634574],\n",
       "       [0.4840679 ],\n",
       "       [0.45454967],\n",
       "       [0.47316656],\n",
       "       [0.47881794],\n",
       "       [0.4658416 ],\n",
       "       [0.5040691 ],\n",
       "       [0.50556797],\n",
       "       [0.49254128],\n",
       "       [0.5079416 ],\n",
       "       [0.48432326],\n",
       "       [0.46701077],\n",
       "       [0.5081134 ],\n",
       "       [0.47036192],\n",
       "       [0.48059693],\n",
       "       [0.5019982 ],\n",
       "       [0.48308745],\n",
       "       [0.4606733 ],\n",
       "       [0.5275587 ],\n",
       "       [0.5021771 ],\n",
       "       [0.49509558],\n",
       "       [0.45899585],\n",
       "       [0.48867655],\n",
       "       [0.49891818],\n",
       "       [0.49377596],\n",
       "       [0.5250295 ],\n",
       "       [0.4910223 ],\n",
       "       [0.46254107],\n",
       "       [0.5110586 ],\n",
       "       [0.48269007],\n",
       "       [0.5331007 ],\n",
       "       [0.49566412],\n",
       "       [0.49817234],\n",
       "       [0.44705424],\n",
       "       [0.51672786],\n",
       "       [0.4972799 ],\n",
       "       [0.5011143 ],\n",
       "       [0.45147288],\n",
       "       [0.4864983 ],\n",
       "       [0.4482205 ],\n",
       "       [0.47523457],\n",
       "       [0.45924103],\n",
       "       [0.49439508],\n",
       "       [0.4578159 ],\n",
       "       [0.45494014],\n",
       "       [0.4944745 ],\n",
       "       [0.5226981 ],\n",
       "       [0.44699377],\n",
       "       [0.458635  ],\n",
       "       [0.47685072],\n",
       "       [0.50229794],\n",
       "       [0.48942643],\n",
       "       [0.45250446],\n",
       "       [0.47907785],\n",
       "       [0.508427  ],\n",
       "       [0.45883608],\n",
       "       [0.50583583],\n",
       "       [0.55432534],\n",
       "       [0.5203651 ],\n",
       "       [0.4502857 ],\n",
       "       [0.45986316],\n",
       "       [0.4860462 ],\n",
       "       [0.52976227],\n",
       "       [0.44949725],\n",
       "       [0.48180023],\n",
       "       [0.5081441 ],\n",
       "       [0.5212655 ],\n",
       "       [0.4848806 ],\n",
       "       [0.5159934 ],\n",
       "       [0.5053235 ],\n",
       "       [0.4802634 ],\n",
       "       [0.49500328],\n",
       "       [0.45996264],\n",
       "       [0.5448042 ],\n",
       "       [0.5471268 ],\n",
       "       [0.4578677 ],\n",
       "       [0.49934646],\n",
       "       [0.4825632 ],\n",
       "       [0.5196381 ],\n",
       "       [0.46061075],\n",
       "       [0.4707405 ],\n",
       "       [0.44928464],\n",
       "       [0.47743702],\n",
       "       [0.46566337],\n",
       "       [0.47989154],\n",
       "       [0.53425604],\n",
       "       [0.47901222],\n",
       "       [0.5485482 ],\n",
       "       [0.5029358 ],\n",
       "       [0.5211879 ],\n",
       "       [0.509977  ],\n",
       "       [0.47163966],\n",
       "       [0.4844513 ],\n",
       "       [0.49786732],\n",
       "       [0.5274473 ],\n",
       "       [0.4766751 ],\n",
       "       [0.51970637],\n",
       "       [0.5126372 ],\n",
       "       [0.48070353],\n",
       "       [0.47248736],\n",
       "       [0.44656703],\n",
       "       [0.45977032],\n",
       "       [0.46490794],\n",
       "       [0.4485998 ],\n",
       "       [0.478774  ],\n",
       "       [0.47506285],\n",
       "       [0.45194697],\n",
       "       [0.4816936 ],\n",
       "       [0.46075746],\n",
       "       [0.50135607],\n",
       "       [0.47491232],\n",
       "       [0.44696945],\n",
       "       [0.46211445],\n",
       "       [0.45509693],\n",
       "       [0.49259308],\n",
       "       [0.50686914],\n",
       "       [0.5268639 ],\n",
       "       [0.46569782],\n",
       "       [0.5119186 ],\n",
       "       [0.5039564 ],\n",
       "       [0.5052242 ],\n",
       "       [0.46187377],\n",
       "       [0.5141039 ],\n",
       "       [0.47497836],\n",
       "       [0.5204792 ],\n",
       "       [0.48630163],\n",
       "       [0.4829935 ],\n",
       "       [0.45370376],\n",
       "       [0.50330687],\n",
       "       [0.5202256 ],\n",
       "       [0.47394964],\n",
       "       [0.47025183],\n",
       "       [0.51807857],\n",
       "       [0.5064935 ],\n",
       "       [0.50395805],\n",
       "       [0.46939877],\n",
       "       [0.4996291 ],\n",
       "       [0.49280947],\n",
       "       [0.4858272 ],\n",
       "       [0.4916933 ],\n",
       "       [0.47374827],\n",
       "       [0.4759371 ],\n",
       "       [0.44940594],\n",
       "       [0.4959383 ],\n",
       "       [0.48531342],\n",
       "       [0.48202452],\n",
       "       [0.47725427],\n",
       "       [0.48459375],\n",
       "       [0.4558465 ],\n",
       "       [0.45863718],\n",
       "       [0.4505098 ],\n",
       "       [0.46521145],\n",
       "       [0.47616416],\n",
       "       [0.49723473],\n",
       "       [0.46483433],\n",
       "       [0.46803227],\n",
       "       [0.50286335],\n",
       "       [0.46459317],\n",
       "       [0.49045023],\n",
       "       [0.4998848 ],\n",
       "       [0.5228414 ],\n",
       "       [0.4884988 ],\n",
       "       [0.48634422],\n",
       "       [0.5236754 ],\n",
       "       [0.54296106],\n",
       "       [0.4862785 ],\n",
       "       [0.501662  ],\n",
       "       [0.5234447 ],\n",
       "       [0.51765865],\n",
       "       [0.48486656],\n",
       "       [0.5052481 ],\n",
       "       [0.48659882],\n",
       "       [0.5291574 ],\n",
       "       [0.4818985 ],\n",
       "       [0.44990343],\n",
       "       [0.50320005],\n",
       "       [0.4819175 ],\n",
       "       [0.45025343],\n",
       "       [0.49141112],\n",
       "       [0.46303794],\n",
       "       [0.4814794 ],\n",
       "       [0.4839855 ],\n",
       "       [0.46380258],\n",
       "       [0.47683045],\n",
       "       [0.4541395 ],\n",
       "       [0.4641322 ],\n",
       "       [0.459832  ],\n",
       "       [0.5059719 ],\n",
       "       [0.50294393],\n",
       "       [0.45370638],\n",
       "       [0.47340882],\n",
       "       [0.46812612],\n",
       "       [0.46939498],\n",
       "       [0.49754608],\n",
       "       [0.45049208],\n",
       "       [0.48746634],\n",
       "       [0.5255298 ],\n",
       "       [0.49616048],\n",
       "       [0.48249847],\n",
       "       [0.47820544],\n",
       "       [0.465808  ],\n",
       "       [0.47252366],\n",
       "       [0.48628744],\n",
       "       [0.46277305],\n",
       "       [0.48212653],\n",
       "       [0.4692085 ],\n",
       "       [0.47509706],\n",
       "       [0.45377263],\n",
       "       [0.4632233 ],\n",
       "       [0.48012176],\n",
       "       [0.48681653],\n",
       "       [0.5348748 ],\n",
       "       [0.45369405],\n",
       "       [0.4588453 ],\n",
       "       [0.45897922],\n",
       "       [0.5433816 ],\n",
       "       [0.47487205],\n",
       "       [0.5335492 ],\n",
       "       [0.4787186 ],\n",
       "       [0.5038058 ],\n",
       "       [0.51241344],\n",
       "       [0.49003217],\n",
       "       [0.51734287],\n",
       "       [0.46689773],\n",
       "       [0.4843085 ],\n",
       "       [0.5191137 ],\n",
       "       [0.520936  ],\n",
       "       [0.5387944 ],\n",
       "       [0.4479751 ],\n",
       "       [0.4986347 ],\n",
       "       [0.49224988],\n",
       "       [0.4976744 ],\n",
       "       [0.48385802],\n",
       "       [0.49600244],\n",
       "       [0.5362686 ],\n",
       "       [0.48857203],\n",
       "       [0.47082618],\n",
       "       [0.4996856 ],\n",
       "       [0.44692194],\n",
       "       [0.4737815 ],\n",
       "       [0.5018471 ],\n",
       "       [0.48289248],\n",
       "       [0.4493692 ],\n",
       "       [0.50674564],\n",
       "       [0.4455885 ],\n",
       "       [0.4967278 ],\n",
       "       [0.44945022],\n",
       "       [0.49250695],\n",
       "       [0.46760887],\n",
       "       [0.46560785],\n",
       "       [0.47392023],\n",
       "       [0.49617147],\n",
       "       [0.5122268 ],\n",
       "       [0.4512894 ],\n",
       "       [0.44463745],\n",
       "       [0.48408154],\n",
       "       [0.49087715],\n",
       "       [0.48514652],\n",
       "       [0.46121466],\n",
       "       [0.49407393],\n",
       "       [0.45993838],\n",
       "       [0.46297166],\n",
       "       [0.48038483],\n",
       "       [0.50746787],\n",
       "       [0.511807  ],\n",
       "       [0.5303917 ],\n",
       "       [0.5109181 ],\n",
       "       [0.47580165],\n",
       "       [0.48060578],\n",
       "       [0.45846838],\n",
       "       [0.491965  ],\n",
       "       [0.5094532 ],\n",
       "       [0.450538  ],\n",
       "       [0.51113266],\n",
       "       [0.46489918],\n",
       "       [0.50839347],\n",
       "       [0.44556543],\n",
       "       [0.49833515],\n",
       "       [0.53314286],\n",
       "       [0.4592285 ],\n",
       "       [0.483818  ],\n",
       "       [0.5163024 ],\n",
       "       [0.47323114],\n",
       "       [0.46134046],\n",
       "       [0.46947563],\n",
       "       [0.48451313],\n",
       "       [0.4851363 ],\n",
       "       [0.47460592],\n",
       "       [0.499009  ],\n",
       "       [0.50203425],\n",
       "       [0.47840878],\n",
       "       [0.48771074],\n",
       "       [0.49844036],\n",
       "       [0.4707103 ],\n",
       "       [0.502274  ],\n",
       "       [0.47798622],\n",
       "       [0.5127129 ],\n",
       "       [0.4721761 ],\n",
       "       [0.45066705],\n",
       "       [0.5052558 ],\n",
       "       [0.46957237],\n",
       "       [0.53422904],\n",
       "       [0.4723429 ],\n",
       "       [0.4914399 ],\n",
       "       [0.5078077 ],\n",
       "       [0.5080706 ],\n",
       "       [0.49271336],\n",
       "       [0.49563858],\n",
       "       [0.49567527],\n",
       "       [0.4680649 ],\n",
       "       [0.50700426],\n",
       "       [0.45083573],\n",
       "       [0.4868939 ],\n",
       "       [0.5020791 ],\n",
       "       [0.46548846],\n",
       "       [0.49750987],\n",
       "       [0.46884233],\n",
       "       [0.48247853],\n",
       "       [0.4737518 ],\n",
       "       [0.50407076],\n",
       "       [0.46125412],\n",
       "       [0.4604181 ],\n",
       "       [0.5052591 ],\n",
       "       [0.4588603 ],\n",
       "       [0.48241016],\n",
       "       [0.47821766],\n",
       "       [0.45366246],\n",
       "       [0.4555773 ],\n",
       "       [0.5004879 ],\n",
       "       [0.47719097],\n",
       "       [0.50033414],\n",
       "       [0.48274487],\n",
       "       [0.45223725],\n",
       "       [0.52520764],\n",
       "       [0.49428308],\n",
       "       [0.48018926],\n",
       "       [0.49432716],\n",
       "       [0.504602  ],\n",
       "       [0.4585083 ],\n",
       "       [0.46872538],\n",
       "       [0.49041605],\n",
       "       [0.498241  ],\n",
       "       [0.49481016],\n",
       "       [0.48352084],\n",
       "       [0.46530226],\n",
       "       [0.48360705],\n",
       "       [0.49941304],\n",
       "       [0.4459035 ],\n",
       "       [0.45857528],\n",
       "       [0.48147443],\n",
       "       [0.5150448 ],\n",
       "       [0.47569832],\n",
       "       [0.46120417],\n",
       "       [0.4876926 ],\n",
       "       [0.47805634],\n",
       "       [0.47003543],\n",
       "       [0.48476174],\n",
       "       [0.50374985],\n",
       "       [0.53052205],\n",
       "       [0.5103295 ],\n",
       "       [0.47135687],\n",
       "       [0.46829575],\n",
       "       [0.4467431 ],\n",
       "       [0.5107429 ],\n",
       "       [0.50728476],\n",
       "       [0.52507496],\n",
       "       [0.47444633],\n",
       "       [0.47840816],\n",
       "       [0.4956136 ],\n",
       "       [0.47877237],\n",
       "       [0.485935  ],\n",
       "       [0.46946168],\n",
       "       [0.46154946],\n",
       "       [0.4735735 ],\n",
       "       [0.47678822],\n",
       "       [0.5397372 ],\n",
       "       [0.46195522],\n",
       "       [0.5163328 ],\n",
       "       [0.509182  ],\n",
       "       [0.48287153],\n",
       "       [0.46990758],\n",
       "       [0.4956735 ],\n",
       "       [0.49115464],\n",
       "       [0.5055026 ],\n",
       "       [0.46110475],\n",
       "       [0.48340863],\n",
       "       [0.46836802],\n",
       "       [0.46051782],\n",
       "       [0.5308395 ],\n",
       "       [0.4953755 ],\n",
       "       [0.45885566],\n",
       "       [0.48854256],\n",
       "       [0.47496584],\n",
       "       [0.45857674],\n",
       "       [0.46516347],\n",
       "       [0.47437662],\n",
       "       [0.48008466],\n",
       "       [0.48266563],\n",
       "       [0.4561194 ],\n",
       "       [0.53437525],\n",
       "       [0.4608303 ],\n",
       "       [0.5108637 ],\n",
       "       [0.5101623 ],\n",
       "       [0.46707666],\n",
       "       [0.5097474 ],\n",
       "       [0.52715987],\n",
       "       [0.47451288],\n",
       "       [0.44339153],\n",
       "       [0.4848603 ],\n",
       "       [0.47312668],\n",
       "       [0.50454944],\n",
       "       [0.46925172],\n",
       "       [0.47438774],\n",
       "       [0.46521825],\n",
       "       [0.4644887 ],\n",
       "       [0.55066174],\n",
       "       [0.46278444],\n",
       "       [0.48328444],\n",
       "       [0.47399938],\n",
       "       [0.46918136],\n",
       "       [0.47482342],\n",
       "       [0.45288464],\n",
       "       [0.4576093 ],\n",
       "       [0.47254032],\n",
       "       [0.47620803],\n",
       "       [0.47065955],\n",
       "       [0.47717392],\n",
       "       [0.49055842],\n",
       "       [0.49792245],\n",
       "       [0.5077139 ],\n",
       "       [0.47381264],\n",
       "       [0.46012124],\n",
       "       [0.471542  ],\n",
       "       [0.5115528 ],\n",
       "       [0.49154654],\n",
       "       [0.5050184 ],\n",
       "       [0.4728785 ],\n",
       "       [0.46806288],\n",
       "       [0.51123184],\n",
       "       [0.46817502],\n",
       "       [0.48678413],\n",
       "       [0.4960482 ],\n",
       "       [0.49069142],\n",
       "       [0.53296936],\n",
       "       [0.4721486 ],\n",
       "       [0.49264267],\n",
       "       [0.5076195 ],\n",
       "       [0.47658455],\n",
       "       [0.49007443],\n",
       "       [0.4591301 ],\n",
       "       [0.46190098],\n",
       "       [0.5034734 ],\n",
       "       [0.47350246],\n",
       "       [0.48935357],\n",
       "       [0.5002904 ],\n",
       "       [0.48815238],\n",
       "       [0.45761985],\n",
       "       [0.49939936],\n",
       "       [0.45050943],\n",
       "       [0.48022914],\n",
       "       [0.485511  ],\n",
       "       [0.46724427],\n",
       "       [0.4873279 ],\n",
       "       [0.47503942],\n",
       "       [0.5360558 ],\n",
       "       [0.4840631 ],\n",
       "       [0.4898824 ],\n",
       "       [0.52868766],\n",
       "       [0.4993099 ],\n",
       "       [0.5060808 ],\n",
       "       [0.5006234 ],\n",
       "       [0.48542088],\n",
       "       [0.47064131],\n",
       "       [0.4901951 ],\n",
       "       [0.46549782],\n",
       "       [0.47216648],\n",
       "       [0.48923677],\n",
       "       [0.45358473],\n",
       "       [0.4523254 ],\n",
       "       [0.46767244],\n",
       "       [0.44665056],\n",
       "       [0.4868839 ],\n",
       "       [0.45977837],\n",
       "       [0.53869224],\n",
       "       [0.48095995],\n",
       "       [0.5387592 ],\n",
       "       [0.5021187 ],\n",
       "       [0.5049913 ],\n",
       "       [0.5347572 ],\n",
       "       [0.47211692],\n",
       "       [0.47620863],\n",
       "       [0.50597066],\n",
       "       [0.4855717 ],\n",
       "       [0.4452935 ],\n",
       "       [0.50626326],\n",
       "       [0.5208289 ],\n",
       "       [0.47270033],\n",
       "       [0.48208976],\n",
       "       [0.48716205],\n",
       "       [0.5150763 ],\n",
       "       [0.47372055],\n",
       "       [0.5044458 ],\n",
       "       [0.46905982],\n",
       "       [0.4627401 ],\n",
       "       [0.49042127],\n",
       "       [0.45762375],\n",
       "       [0.5378638 ],\n",
       "       [0.500961  ],\n",
       "       [0.51587504],\n",
       "       [0.4997825 ],\n",
       "       [0.47778556],\n",
       "       [0.5020249 ],\n",
       "       [0.4757251 ],\n",
       "       [0.47115412],\n",
       "       [0.46227354],\n",
       "       [0.4819689 ],\n",
       "       [0.5192302 ],\n",
       "       [0.48350036],\n",
       "       [0.48818845],\n",
       "       [0.51771057],\n",
       "       [0.46893135],\n",
       "       [0.5136991 ],\n",
       "       [0.4735065 ],\n",
       "       [0.48450145],\n",
       "       [0.4710711 ],\n",
       "       [0.47295845],\n",
       "       [0.46732056],\n",
       "       [0.47264072],\n",
       "       [0.48996228],\n",
       "       [0.47961614],\n",
       "       [0.46413493],\n",
       "       [0.49993405],\n",
       "       [0.47473338],\n",
       "       [0.50618094],\n",
       "       [0.5100247 ],\n",
       "       [0.4619606 ],\n",
       "       [0.49482074],\n",
       "       [0.51767105],\n",
       "       [0.49630603],\n",
       "       [0.4773042 ],\n",
       "       [0.49030125],\n",
       "       [0.45784214],\n",
       "       [0.4511726 ],\n",
       "       [0.49224475],\n",
       "       [0.4474112 ],\n",
       "       [0.5011883 ],\n",
       "       [0.46289933],\n",
       "       [0.5022109 ],\n",
       "       [0.47916523],\n",
       "       [0.47775027],\n",
       "       [0.49112672],\n",
       "       [0.46933612],\n",
       "       [0.50132203],\n",
       "       [0.45762476],\n",
       "       [0.46012124],\n",
       "       [0.4740328 ],\n",
       "       [0.5145873 ],\n",
       "       [0.49137002],\n",
       "       [0.47974294],\n",
       "       [0.470866  ],\n",
       "       [0.46278834],\n",
       "       [0.4820466 ],\n",
       "       [0.44235694],\n",
       "       [0.45864323],\n",
       "       [0.46682703],\n",
       "       [0.4953732 ],\n",
       "       [0.5122171 ],\n",
       "       [0.5557923 ],\n",
       "       [0.5213105 ],\n",
       "       [0.46701548],\n",
       "       [0.50062656],\n",
       "       [0.47562894],\n",
       "       [0.4700861 ],\n",
       "       [0.49297643],\n",
       "       [0.4645938 ],\n",
       "       [0.4885297 ],\n",
       "       [0.520711  ],\n",
       "       [0.44751352],\n",
       "       [0.49117497],\n",
       "       [0.4887078 ],\n",
       "       [0.46966216],\n",
       "       [0.53290844],\n",
       "       [0.5349225 ],\n",
       "       [0.52248263],\n",
       "       [0.4742453 ],\n",
       "       [0.4821577 ],\n",
       "       [0.4772591 ],\n",
       "       [0.4557625 ],\n",
       "       [0.4536313 ],\n",
       "       [0.4890092 ],\n",
       "       [0.46917537],\n",
       "       [0.4512988 ],\n",
       "       [0.49862093],\n",
       "       [0.45330167],\n",
       "       [0.48478022],\n",
       "       [0.47681743],\n",
       "       [0.48043627],\n",
       "       [0.5107198 ],\n",
       "       [0.4764293 ],\n",
       "       [0.48789534],\n",
       "       [0.5304718 ],\n",
       "       [0.45277795],\n",
       "       [0.5005342 ],\n",
       "       [0.46733263],\n",
       "       [0.46306708],\n",
       "       [0.446265  ],\n",
       "       [0.46841112],\n",
       "       [0.4583474 ],\n",
       "       [0.4883731 ],\n",
       "       [0.52905154],\n",
       "       [0.4793115 ],\n",
       "       [0.49740243],\n",
       "       [0.44723693],\n",
       "       [0.4681678 ],\n",
       "       [0.46999714],\n",
       "       [0.49417254],\n",
       "       [0.48480344],\n",
       "       [0.5071804 ],\n",
       "       [0.50921655],\n",
       "       [0.46575582],\n",
       "       [0.46415234],\n",
       "       [0.48723146],\n",
       "       [0.45972618],\n",
       "       [0.46089098],\n",
       "       [0.5052756 ],\n",
       "       [0.48766553],\n",
       "       [0.503862  ],\n",
       "       [0.50614387],\n",
       "       [0.4946159 ],\n",
       "       [0.49073482],\n",
       "       [0.47912928],\n",
       "       [0.46056947],\n",
       "       [0.49100196],\n",
       "       [0.49046972],\n",
       "       [0.48654577],\n",
       "       [0.4826018 ],\n",
       "       [0.5074695 ],\n",
       "       [0.4821014 ],\n",
       "       [0.46681923],\n",
       "       [0.4616884 ],\n",
       "       [0.4957784 ],\n",
       "       [0.45527065],\n",
       "       [0.46775922],\n",
       "       [0.48831925],\n",
       "       [0.52400625],\n",
       "       [0.5440875 ],\n",
       "       [0.48074812],\n",
       "       [0.4891468 ],\n",
       "       [0.45521748],\n",
       "       [0.44630173],\n",
       "       [0.45712143],\n",
       "       [0.46168151],\n",
       "       [0.5298371 ],\n",
       "       [0.4821118 ],\n",
       "       [0.45338517],\n",
       "       [0.4797142 ],\n",
       "       [0.50560963],\n",
       "       [0.5546724 ],\n",
       "       [0.48615476],\n",
       "       [0.5043889 ],\n",
       "       [0.45030344],\n",
       "       [0.4959386 ],\n",
       "       [0.4783604 ],\n",
       "       [0.4583227 ],\n",
       "       [0.46408275],\n",
       "       [0.5147114 ],\n",
       "       [0.46649548],\n",
       "       [0.5100055 ],\n",
       "       [0.49187657],\n",
       "       [0.51211107],\n",
       "       [0.5113204 ],\n",
       "       [0.52181953],\n",
       "       [0.46544152],\n",
       "       [0.46097788],\n",
       "       [0.47484258],\n",
       "       [0.46871805],\n",
       "       [0.48430985],\n",
       "       [0.50676537],\n",
       "       [0.46721822],\n",
       "       [0.47368217],\n",
       "       [0.473162  ],\n",
       "       [0.50276786],\n",
       "       [0.4490611 ],\n",
       "       [0.495598  ],\n",
       "       [0.525314  ],\n",
       "       [0.4969225 ],\n",
       "       [0.5321312 ],\n",
       "       [0.47219568],\n",
       "       [0.52469057],\n",
       "       [0.47635034],\n",
       "       [0.5507224 ],\n",
       "       [0.5165776 ],\n",
       "       [0.49510828],\n",
       "       [0.4470843 ],\n",
       "       [0.50575054],\n",
       "       [0.46680105],\n",
       "       [0.47759414],\n",
       "       [0.47917104],\n",
       "       [0.47877154],\n",
       "       [0.49821535],\n",
       "       [0.4440172 ],\n",
       "       [0.48592892],\n",
       "       [0.47408348],\n",
       "       [0.52457285],\n",
       "       [0.49223277],\n",
       "       [0.49820453],\n",
       "       [0.5004618 ],\n",
       "       [0.49764374],\n",
       "       [0.4556509 ],\n",
       "       [0.49531993],\n",
       "       [0.50134206],\n",
       "       [0.44713032],\n",
       "       [0.47049415],\n",
       "       [0.48664588],\n",
       "       [0.46116915],\n",
       "       [0.4983705 ],\n",
       "       [0.4787068 ],\n",
       "       [0.47509548],\n",
       "       [0.5325758 ],\n",
       "       [0.5003155 ],\n",
       "       [0.47266915],\n",
       "       [0.4645556 ],\n",
       "       [0.5016729 ],\n",
       "       [0.47223565],\n",
       "       [0.4979597 ],\n",
       "       [0.5233199 ],\n",
       "       [0.47818565],\n",
       "       [0.50295323],\n",
       "       [0.5296548 ],\n",
       "       [0.54056704],\n",
       "       [0.48485252],\n",
       "       [0.4480942 ],\n",
       "       [0.5124179 ],\n",
       "       [0.498481  ],\n",
       "       [0.48823655],\n",
       "       [0.47593698],\n",
       "       [0.5171961 ],\n",
       "       [0.45876226],\n",
       "       [0.53643626],\n",
       "       [0.4486186 ],\n",
       "       [0.46509162],\n",
       "       [0.47851834],\n",
       "       [0.5055843 ],\n",
       "       [0.5456797 ],\n",
       "       [0.53830504],\n",
       "       [0.49144757],\n",
       "       [0.5410539 ],\n",
       "       [0.5166132 ],\n",
       "       [0.4772471 ],\n",
       "       [0.45327905],\n",
       "       [0.502233  ],\n",
       "       [0.52883106],\n",
       "       [0.48511845],\n",
       "       [0.53340995],\n",
       "       [0.49247664],\n",
       "       [0.47488055],\n",
       "       [0.51038986],\n",
       "       [0.46236935],\n",
       "       [0.5131913 ],\n",
       "       [0.45666352],\n",
       "       [0.514071  ],\n",
       "       [0.4754902 ],\n",
       "       [0.49683222],\n",
       "       [0.45822194],\n",
       "       [0.46940252],\n",
       "       [0.49937454],\n",
       "       [0.47395617],\n",
       "       [0.47079295],\n",
       "       [0.51850307],\n",
       "       [0.48780653],\n",
       "       [0.5122533 ],\n",
       "       [0.5235521 ],\n",
       "       [0.44554335],\n",
       "       [0.47771344],\n",
       "       [0.4579948 ],\n",
       "       [0.4461881 ],\n",
       "       [0.47220507],\n",
       "       [0.47397438],\n",
       "       [0.47635397],\n",
       "       [0.46534175],\n",
       "       [0.45805657],\n",
       "       [0.44622737],\n",
       "       [0.46624988],\n",
       "       [0.44782963],\n",
       "       [0.45249152],\n",
       "       [0.51964873],\n",
       "       [0.48701757],\n",
       "       [0.5110537 ],\n",
       "       [0.48850128],\n",
       "       [0.53157705],\n",
       "       [0.4803056 ],\n",
       "       [0.4475108 ],\n",
       "       [0.47337756],\n",
       "       [0.4481343 ],\n",
       "       [0.47991142],\n",
       "       [0.4579267 ],\n",
       "       [0.48722076],\n",
       "       [0.48722732],\n",
       "       [0.5001598 ],\n",
       "       [0.49111086],\n",
       "       [0.45935145],\n",
       "       [0.48771402],\n",
       "       [0.49308452],\n",
       "       [0.48914102],\n",
       "       [0.47767234],\n",
       "       [0.45841894],\n",
       "       [0.48954216],\n",
       "       [0.5081802 ],\n",
       "       [0.5251996 ],\n",
       "       [0.47840238],\n",
       "       [0.5249712 ],\n",
       "       [0.45692894],\n",
       "       [0.4599478 ],\n",
       "       [0.49141023],\n",
       "       [0.5178498 ],\n",
       "       [0.47067225],\n",
       "       [0.45950034],\n",
       "       [0.5182844 ],\n",
       "       [0.47535083],\n",
       "       [0.46452436],\n",
       "       [0.5386198 ],\n",
       "       [0.48232234],\n",
       "       [0.5047087 ],\n",
       "       [0.5094448 ],\n",
       "       [0.44655868],\n",
       "       [0.46368033],\n",
       "       [0.5029227 ],\n",
       "       [0.4803555 ],\n",
       "       [0.49510115],\n",
       "       [0.4878028 ],\n",
       "       [0.4816718 ],\n",
       "       [0.50013506],\n",
       "       [0.49571496],\n",
       "       [0.47260314],\n",
       "       [0.4515635 ],\n",
       "       [0.47056708],\n",
       "       [0.4618825 ],\n",
       "       [0.5268076 ],\n",
       "       [0.5007931 ],\n",
       "       [0.4951248 ],\n",
       "       [0.45857653],\n",
       "       [0.48078203],\n",
       "       [0.4652124 ],\n",
       "       [0.48984617],\n",
       "       [0.4892738 ],\n",
       "       [0.5019038 ],\n",
       "       [0.4972391 ],\n",
       "       [0.48374474],\n",
       "       [0.44531694],\n",
       "       [0.50119054],\n",
       "       [0.45701766],\n",
       "       [0.48335636],\n",
       "       [0.5038985 ],\n",
       "       [0.4769532 ],\n",
       "       [0.47254136],\n",
       "       [0.5434295 ],\n",
       "       [0.51812994],\n",
       "       [0.46524194],\n",
       "       [0.4652951 ],\n",
       "       [0.4902258 ],\n",
       "       [0.4639002 ],\n",
       "       [0.48318714],\n",
       "       [0.4935779 ],\n",
       "       [0.52591246],\n",
       "       [0.47908583],\n",
       "       [0.47382453],\n",
       "       [0.52304226]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformation():\n",
    "    \"\"\"\n",
    "    Do an inference on a single batch of data. In this sample server, we take\n",
    "    data as CSV, convert it to a pandas data frame for internal use and then\n",
    "    convert the predictions back to CSV (which really just means one prediction\n",
    "    per line, since there's a single column.\n",
    "    \"\"\"\n",
    "    data = None\n",
    "\n",
    "    # Convert from CSV to pandas\n",
    "    s = 'df_test.csv'   # MODIFIED\n",
    "    data = pd.read_csv(s, header=None)\n",
    "    data = transform_data(data)\n",
    "    # Do the prediction\n",
    "    predictions = ScoringService.predict(data)\n",
    "    return predictions   # MODIFIED\n",
    "\n",
    "\n",
    "transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version   \r\n",
      "---------------------------------- ----------\r\n",
      "absl-py                            0.9.0     \r\n",
      "alabaster                          0.7.10    \r\n",
      "anaconda-client                    1.6.14    \r\n",
      "anaconda-project                   0.8.2     \r\n",
      "asn1crypto                         0.24.0    \r\n",
      "astor                              0.8.1     \r\n",
      "astroid                            1.6.3     \r\n",
      "astropy                            3.0.2     \r\n",
      "attrs                              18.1.0    \r\n",
      "Automat                            0.3.0     \r\n",
      "autovizwidget                      0.15.0    \r\n",
      "awscli                             1.18.39   \r\n",
      "Babel                              2.5.3     \r\n",
      "backcall                           0.1.0     \r\n",
      "backports.shutil-get-terminal-size 1.0.0     \r\n",
      "bcrypt                             3.1.7     \r\n",
      "beautifulsoup4                     4.6.0     \r\n",
      "bitarray                           0.8.1     \r\n",
      "bkcharts                           0.2       \r\n",
      "blaze                              0.11.3    \r\n",
      "bleach                             2.1.3     \r\n",
      "bokeh                              1.4.0     \r\n",
      "boto                               2.48.0    \r\n",
      "boto3                              1.12.39   \r\n",
      "botocore                           1.15.39   \r\n",
      "Bottleneck                         1.2.1     \r\n",
      "cached-property                    1.5.1     \r\n",
      "certifi                            2019.11.28\r\n",
      "cffi                               1.11.5    \r\n",
      "characteristic                     14.3.0    \r\n",
      "chardet                            3.0.4     \r\n",
      "click                              6.7       \r\n",
      "cloudpickle                        0.5.3     \r\n",
      "clyent                             1.2.2     \r\n",
      "colorama                           0.3.9     \r\n",
      "contextlib2                        0.5.5     \r\n",
      "cryptography                       2.9       \r\n",
      "cycler                             0.10.0    \r\n",
      "Cython                             0.28.4    \r\n",
      "cytoolz                            0.9.0.1   \r\n",
      "dask                               1.2.2     \r\n",
      "datashape                          0.5.4     \r\n",
      "decorator                          4.3.0     \r\n",
      "defusedxml                         0.6.0     \r\n",
      "distributed                        1.28.1    \r\n",
      "docker                             4.2.0     \r\n",
      "docker-compose                     1.25.5    \r\n",
      "dockerpty                          0.4.1     \r\n",
      "docopt                             0.6.2     \r\n",
      "docutils                           0.14      \r\n",
      "entrypoints                        0.2.3     \r\n",
      "enum34                             1.1.9     \r\n",
      "environment-kernels                1.1.1     \r\n",
      "et-xmlfile                         1.0.1     \r\n",
      "fastcache                          1.0.2     \r\n",
      "filelock                           3.0.4     \r\n",
      "Flask                              1.0.2     \r\n",
      "Flask-Cors                         3.0.4     \r\n",
      "future                             0.18.2    \r\n",
      "gast                               0.2.2     \r\n",
      "gevent                             1.3.0     \r\n",
      "glob2                              0.6       \r\n",
      "gmpy2                              2.0.8     \r\n",
      "google-pasta                       0.1.8     \r\n",
      "greenlet                           0.4.13    \r\n",
      "grpcio                             1.10.1    \r\n",
      "h5py                               2.8.0     \r\n",
      "hdijupyterutils                    0.15.0    \r\n",
      "heapdict                           1.0.0     \r\n",
      "horovod                            0.19.0    \r\n",
      "html5lib                           1.0.1     \r\n",
      "hyperas                            0.4.1     \r\n",
      "hyperopt                           0.2.4     \r\n",
      "idna                               2.6       \r\n",
      "imageio                            2.3.0     \r\n",
      "imagesize                          1.0.0     \r\n",
      "importlib-metadata                 1.5.0     \r\n",
      "ipykernel                          4.8.2     \r\n",
      "ipyparallel                        6.2.2     \r\n",
      "ipython                            6.4.0     \r\n",
      "ipython-genutils                   0.2.0     \r\n",
      "ipywidgets                         7.4.0     \r\n",
      "isort                              4.3.4     \r\n",
      "itsdangerous                       0.24      \r\n",
      "jdcal                              1.4       \r\n",
      "jedi                               0.12.0    \r\n",
      "Jinja2                             2.10      \r\n",
      "jmespath                           0.9.4     \r\n",
      "joblib                             0.14.1    \r\n",
      "jsonschema                         2.6.0     \r\n",
      "jupyter                            1.0.0     \r\n",
      "jupyter-client                     5.2.3     \r\n",
      "jupyter-console                    5.2.0     \r\n",
      "jupyter-core                       4.4.0     \r\n",
      "jupyterlab                         0.32.1    \r\n",
      "jupyterlab-launcher                0.10.5    \r\n",
      "Keras                              2.2.4     \r\n",
      "Keras-Applications                 1.0.8     \r\n",
      "Keras-Preprocessing                1.1.0     \r\n",
      "kiwisolver                         1.0.1     \r\n",
      "lazy-object-proxy                  1.3.1     \r\n",
      "llvmlite                           0.23.1    \r\n",
      "locket                             0.2.0     \r\n",
      "lxml                               4.2.1     \r\n",
      "Markdown                           3.2.1     \r\n",
      "MarkupSafe                         1.0       \r\n",
      "matplotlib                         3.0.3     \r\n",
      "mccabe                             0.6.1     \r\n",
      "mistune                            0.8.3     \r\n",
      "mkl-fft                            1.0.15    \r\n",
      "mkl-random                         1.1.0     \r\n",
      "mkl-service                        2.3.0     \r\n",
      "mock                               4.0.1     \r\n",
      "more-itertools                     4.1.0     \r\n",
      "mpmath                             1.0.0     \r\n",
      "msgpack                            0.6.0     \r\n",
      "msgpack-python                     0.5.6     \r\n",
      "multipledispatch                   0.5.0     \r\n",
      "nb-conda                           2.2.1     \r\n",
      "nb-conda-kernels                   2.2.2     \r\n",
      "nbconvert                          5.4.1     \r\n",
      "nbformat                           4.4.0     \r\n",
      "networkx                           2.4       \r\n",
      "nltk                               3.3       \r\n",
      "nose                               1.3.7     \r\n",
      "notebook                           5.5.0     \r\n",
      "numba                              0.38.0    \r\n",
      "numexpr                            2.7.1     \r\n",
      "numpy                              1.16.4    \r\n",
      "numpydoc                           0.8.0     \r\n",
      "odo                                0.5.1     \r\n",
      "olefile                            0.45.1    \r\n",
      "opencv-python                      3.4.2.17  \r\n",
      "openpyxl                           2.5.3     \r\n",
      "opt-einsum                         3.1.0     \r\n",
      "packaging                          20.1      \r\n",
      "pandas                             0.24.2    \r\n",
      "pandocfilters                      1.4.2     \r\n",
      "paramiko                           2.7.1     \r\n",
      "parso                              0.2.0     \r\n",
      "partd                              0.3.8     \r\n",
      "path.py                            11.0.1    \r\n",
      "pathlib2                           2.3.2     \r\n",
      "patsy                              0.5.0     \r\n",
      "pep8                               1.7.1     \r\n",
      "pexpect                            4.5.0     \r\n",
      "pickleshare                        0.7.4     \r\n",
      "Pillow                             5.4.1     \r\n",
      "pip                                19.3.1    \r\n",
      "pkginfo                            1.4.2     \r\n",
      "plotly                             4.5.2     \r\n",
      "pluggy                             0.6.0     \r\n",
      "ply                                3.11      \r\n",
      "prompt-toolkit                     1.0.15    \r\n",
      "protobuf                           3.8.0     \r\n",
      "protobuf3-to-dict                  0.1.5     \r\n",
      "psutil                             5.4.5     \r\n",
      "psycopg2                           2.7.5     \r\n",
      "ptyprocess                         0.5.2     \r\n",
      "py                                 1.5.3     \r\n",
      "py4j                               0.10.7    \r\n",
      "pyaml                              20.4.0    \r\n",
      "pyasn1                             0.4.8     \r\n",
      "pycodestyle                        2.4.0     \r\n",
      "pycosat                            0.6.3     \r\n",
      "pycparser                          2.18      \r\n",
      "pycrypto                           2.6.1     \r\n",
      "pycurl                             7.43.0.2  \r\n",
      "pyflakes                           1.6.0     \r\n",
      "pygal                              2.4.0     \r\n",
      "Pygments                           2.2.0     \r\n",
      "pykerberos                         1.2.1     \r\n",
      "pylint                             1.8.4     \r\n",
      "PyNaCl                             1.3.0     \r\n",
      "pyodbc                             4.0.23    \r\n",
      "pyOpenSSL                          18.0.0    \r\n",
      "pyparsing                          2.2.0     \r\n",
      "PySocks                            1.6.8     \r\n",
      "pyspark                            2.3.4     \r\n",
      "pytest                             3.5.1     \r\n",
      "pytest-arraydiff                   0.2       \r\n",
      "pytest-astropy                     0.3.0     \r\n",
      "pytest-doctestplus                 0.1.3     \r\n",
      "pytest-openfiles                   0.3.0     \r\n",
      "pytest-remotedata                  0.2.1     \r\n",
      "python-dateutil                    2.7.3     \r\n",
      "pytz                               2018.4    \r\n",
      "PyWavelets                         0.5.2     \r\n",
      "PyYAML                             5.3.1     \r\n",
      "pyzmq                              17.0.0    \r\n",
      "QtAwesome                          0.4.4     \r\n",
      "qtconsole                          4.3.1     \r\n",
      "QtPy                               1.4.1     \r\n",
      "requests                           2.20.0    \r\n",
      "requests-kerberos                  0.12.0    \r\n",
      "retrying                           1.3.3     \r\n",
      "rope                               0.10.7    \r\n",
      "rsa                                3.4.2     \r\n",
      "ruamel-yaml                        0.15.35   \r\n",
      "s3fs                               0.1.5     \r\n",
      "s3transfer                         0.3.3     \r\n",
      "sagemaker                          1.55.3    \r\n",
      "sagemaker-pyspark                  1.3.0     \r\n",
      "scikit-image                       0.13.1    \r\n",
      "scikit-learn                       0.22.1    \r\n",
      "scikit-optimize                    0.7.4     \r\n",
      "scipy                              1.4.1     \r\n",
      "seaborn                            0.8.1     \r\n",
      "Send2Trash                         1.5.0     \r\n",
      "setuptools                         45.2.0    \r\n",
      "simplegeneric                      0.8.1     \r\n",
      "singledispatch                     3.4.0.3   \r\n",
      "six                                1.11.0    \r\n",
      "smdebug-rulesconfig                0.1.2     \r\n",
      "snowballstemmer                    1.2.1     \r\n",
      "sortedcollections                  0.6.1     \r\n",
      "sortedcontainers                   1.5.10    \r\n",
      "sparkmagic                         0.12.5    \r\n",
      "Sphinx                             1.7.4     \r\n",
      "sphinxcontrib-websupport           1.0.1     \r\n",
      "spyder                             3.2.8     \r\n",
      "SQLAlchemy                         1.2.11    \r\n",
      "statsmodels                        0.9.0     \r\n",
      "sympy                              1.1.1     \r\n",
      "tables                             3.4.3     \r\n",
      "TBB                                0.1       \r\n",
      "tblib                              1.3.2     \r\n",
      "tensorboard                        1.15.0    \r\n",
      "tensorflow                         1.15.2    \r\n",
      "tensorflow-estimator               1.15.1    \r\n",
      "tensorflow-serving-api             1.15.0    \r\n",
      "termcolor                          1.1.0     \r\n",
      "terminado                          0.8.1     \r\n",
      "testpath                           0.3.1     \r\n",
      "texttable                          1.6.2     \r\n",
      "toolz                              0.9.0     \r\n",
      "tornado                            5.0.2     \r\n",
      "tqdm                               4.46.1    \r\n",
      "traitlets                          4.3.2     \r\n",
      "typing                             3.6.4     \r\n",
      "unicodecsv                         0.14.1    \r\n",
      "urllib3                            1.23      \r\n",
      "wcwidth                            0.1.7     \r\n",
      "webencodings                       0.5.1     \r\n",
      "websocket-client                   0.57.0    \r\n",
      "Werkzeug                           0.14.1    \r\n",
      "wheel                              0.31.1    \r\n",
      "widgetsnbextension                 3.4.2     \r\n",
      "wrapt                              1.11.2    \r\n",
      "xlrd                               1.1.0     \r\n",
      "XlsxWriter                         1.0.4     \r\n",
      "xlwt                               1.3.0     \r\n",
      "zict                               0.1.3     \r\n",
      "zipp                               3.0.0     \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
